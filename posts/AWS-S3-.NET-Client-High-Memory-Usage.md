<p>Reducing AWS S3&#xA0;.NET client LOH allocations by&#xA0;98%</p><h3>Contents</h3><ul><li><a href="https://medium.com/p/c06fac8784df#2dad">Problem discovery</a></li><li><a href="https://medium.com/p/c06fac8784df#27e8">Why is it a&#xA0;problem?</a></li><li><a href="https://medium.com/p/c06fac8784df#299f">Introducing the best magic number&#x200A;&#x2014;&#x200A;81,920</a></li><li><a href="https://medium.com/p/c06fac8784df#354b">Idle hands</a></li><li><a href="https://medium.com/p/c06fac8784df#35cc">Just one more&#xA0;thing</a></li><li><a href="https://medium.com/p/c06fac8784df#1129">TLDR&#x200A;&#x2014;&#x200A;Give me the good&#xA0;stuff</a></li><li><a href="https://medium.com/p/c06fac8784df#0c77">Footnotes</a></li></ul><h3>Problem discovery</h3><p>One of the things we do at <a href="https://codeweavers.net/">Codeweavers</a> is help people find their next vehicle. That usually involves customers seeing what vehicle they are buying&#x200A;&#x2014;&#x200A;I mean, would you buy a car without seeing what it looks like? The application that holds this responsibility is <em>the</em> worst offender for obscene amounts of allocations, time spent in GC, and generally eating RAM like the Cookie Monster eats well&#x2026;cookies.</p><p>Every now and then we like to take a memory dump of this application from our production environment. We have done this enough times that we have automated the most common diagnostics steps we take and bundled them into a little tool&#xB9; called ADA (Automated Dump Analysis). If you are interested you can find the tool <a href="https://github.com/indy-singh/AutomatedDumpAnalysis">here</a> and all the code talked about in this article&#xA0;<a href="https://github.com/indy-singh/ItsNotYourCode-AWS-S3">here</a>.</p><p>One of the analysers that we run is to dump all the byte[] arrays found on the Large Object Heap (LOH). After running that analyser against our eight gigabyte memory dump, we found several hundred byte[] arrays with a length of 131,096 or 131,186. Well that is pretty odd. Opening some of the files in <a href="https://notepad-plus-plus.org/">Notepad++</a> just presented us with lots of random characters.</p><p>Throwing the <a href="https://en.wikipedia.org/wiki/Scientific_method">scientific method</a> out of the window for a second, I decided to mass rename all the dumped byte[] arrays to *.jpg - hey presto some of the files were now displaying thumbnails! On closer inspection, around 50% of the files were images. The other 50% failed to open as an image at all. Opening a handful of the non-image files in Notepad++ showed that they all had a line similar to this right at the beginning of the&#xA0;file:-</p><pre><code class="language-markup">0;chunk-signature=48ebf1394fcc452801d4ccebf0598177c7b31876e3fbcb7f6156213f931b261d</code></pre><p>Okay, this is beginning to make a little more sense. The byte[] arrays that have a length of 131,096 are pure images. The byte[] arrays that are not images have a length of 131,186 and have a chunk-signature line before the rest of the contents. I would <em>guess</em> the signature is a SHA256 hash of the contents.</p><p>Before we go any further, it is worth establishing how busy this application is with image processing. All of our image processing is distributed across our farm using AWS <a href="https://aws.amazon.com/sns/">SNS</a> and <a href="https://aws.amazon.com/sqs/">SQS</a>. Using <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html">CloudWatch Metrics</a> we can see that&#xA0;easily:-</p><figure><img alt src="https://hackernoon.com/hn-images/1*CA-xDIDiLPuBVaH-gI5FFQ.png"></figure><p>Okay, so <em>fairly</em> busy. It is worth noting that before <em>any</em> performance centric work is carried out, always establish how often the code is hit and the current costs. If a code path has a high cost (e.g. takes twenty seconds) but is only hit once a day, then it is not worth investigating. However, if the same code path is hit a lot (e.g. a million times a day) then it is definitely worth investigating.</p><p>At this point I had two culprits in mind. We have already established the application in question does a lot of image processing. But there are a few moving parts and two ways of kicking off the image processing:-</p><ol><li>Images are pushed to&#xA0;us</li><li>We pull images from a&#xA0;SFTP</li></ol><p>After that we transform the image and then upload it to <a href="https://aws.amazon.com/s3/">AWS S3</a>. At this stage I was leaning towards the SFTP, as it probably needed to verify each chunk it received from the server. But following my hunches has lead me on a wild goose chase before so ignoring my hunch I plugged chunk-signature into Google and smashed enter. Google pointed to AWS S3 as <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-streaming.html#sigv4-chunked-body-definition">the culprit</a>. But that is just theory, we need to prove&#xA0;that.</p><iframe src width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f15095d201b5b069bd5fcbd95eeca6b2/href">https://medium.com/media/f15095d201b5b069bd5fcbd95eeca6b2/href</a></iframe><p>If we upload the <a href="https://a0.awsstatic.com/main/images/logos/aws_logo_smile_1200x630.png">same image</a> ten times and use <a href="https://www.jetbrains.com/profiler/">dotTrace</a> to view the LOH we see an interesting pattern:-</p><figure><img alt src="https://hackernoon.com/hn-images/0*0nJkFG0KOqCUoFSC.png"></figure><p>It looks like every time we call PutObject on the AWS S3&#xA0;.NET client there is a fixed cost of 0.3 MB. This is a problem because it means every time you use PutObject, you are paying a high cost of 0.3 MB per upload. Just to make sure; what happens if we increase the number of times we upload from ten times to one hundred&#xA0;times?</p><figure><img alt src="https://hackernoon.com/hn-images/0*z1XLvRm5kxc5_m8D.png"></figure><p>Yes, we can definitely say that for every invocation of PutObject a costly allocation of 0.3 MB is made. Going one step further and dumping the process using <a href="https://docs.microsoft.com/en-us/sysinternals/downloads/procdump">ProcDump</a>:-</p><pre><code class="language-markup">procdump64.exe -ma -64 AWS-S3.exe</code></pre><p>Running the dump file through ADA we see the exact same characteristics of there being two groups of byte[] arrays; 50% have a length 131,096 and the other 50% have a length of 131,186. Half the files are images when renamed, and half the files have the chunk-signature starting line. At this point we are certain that the AWS S3&#xA0;.NET client is allocating byte[] arrays directly onto the LOH; and that <em>is</em> a&#xA0;problem.</p><h3>Why is it a&#xA0;problem?</h3><p>The LOH is a region of memory that is <em>collected</em> but never <em>compacted</em>&#x200A;&#x2014;&#x200A;though as of&#xA0;.NET v4.5.1 <a href="https://docs.microsoft.com/en-gb/dotnet/api/system.runtime.gclargeobjectheapcompactionmode">compaction is now possible</a>&#x200A;&#x2014;&#x200A;word of warning compaction of the LOH is expensive; around <a href="https://youtu.be/adY0KZ3bf8I?t=5m25s">2.3 milliseconds per megabyte</a>. A good rule of thumb is that short-lived objects should <em>never</em> make it onto the&#xA0;LOH.</p><p>Objects that are equal to or greater than <a href="https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/large-object-heap">85,000</a> bytes go straight onto the LOH. The LOH operates very differently from other regions of memory. Other regions of memory get collected and compacted regularly meaning that you can just add new objects to the end after the garbage collector runs. Whereas the LOH tries to fit newly allocated objects in free space left after dead objects are discarded. This works fine if the newly allocated object is the exact same size or smaller as the free space. If a space can not be found then the LOH has to grow to accommodate that&#xA0;object.</p><p>It helps to think of it like a bookshelf; in other regions of memory, books that are no longer used are simply thrown away and the remaining books are pushed together and any new books go at the end of the book&#xA0;shelf.</p><p>Within the LOH that is not possible, instead books (objects) are thrown away, and the number of pages that used to be in that space (bytes) is recorded and the next time a book gets allocated to the that shelf (the LOH) it attempts to find an empty space that can hold that many pages (bytes). If the shelf can not accommodate the newly allocated book (object) then the shelf must be extended to hold that new book (object).</p><p>The garbage collector will collect dead objects from the LOH, and in the mean time new objects are being allocated to the LOH. This can lead to a situation over a lifetime of a long running application where the LOH size has grown to a few gigabytes (because new objects did not fit into existing empty space) but actually only contains a few alive objects. This is known as LOH fragmentation. We were <em>extremely</em> lucky in this situation as the byte[] arrays that made it onto the LOH had two sizes; 131,186 and 131,096. This means that as old objects of either size died and were collected, newly allocated objects were just the right size to slot right into the empty&#xA0;space.</p><p>Okay, back to the fun&#xA0;stuff.</p><h3>Introducing the best magic number&#x200A;&#x2014;&#x200A;81,920</h3><p>Thanks to dotTrace we were able to establish exactly what was causing the LOH fragmentation. It also showed us that the fixed cost of 0.3 MB per invocation of PutObject happened inside of the constructor for ChunkedUploadWrapperStream:-</p><figure><img alt src="https://hackernoon.com/hn-images/0*hRpfqCV3YK-vfRKN.JPG"></figure><p>A quick visit to <a href="https://github.com/aws/aws-sdk-net/blob/9814e245e0cd9c4435a17185e866813869d427da/sdk/src/Core/Amazon.Runtime/Internal/Util/ChunkedUploadWrapperStream.cs#L73">that file</a> in aws-sdk-net repository. Shows that two byte[] arrays are created with a length of at least 131,072:-</p><iframe src width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2309e8b4dbb4e731387d7636c5ea73d4/href">https://medium.com/media/2309e8b4dbb4e731387d7636c5ea73d4/href</a></iframe><p>This is exactly why these byte[] arrays are allocated directly to the LOH, they are above the LOH threshold (85,000 bytes). At this point there are a few possible solutions:-</p><ol><li>Use <a href="http://adamsitnik.com/Array-Pool/">System.Buffers</a> to rent two byte[] arrays from a pool of byte[]&#xA0;arrays</li><li>Use <a href="http://www.philosophicalgeek.com/2015/02/06/announcing-microsoft-io-recycablememorystream/">Microsoft.IO.RecycableMemoryStream</a> and operate directly on the incoming stream using a pool of&#xA0;Stream&apos;s</li><li>Expose DefaultChunkSize so that consumers of the API can set it themselves</li><li>Lower DefaultChunkSize to a number that is below LOH threshold (85,000&#xA0;bytes)</li></ol><p>The first and second solutions are probably the ones with the biggest wins to be had, but that would require a large pull request and introducing a dependency that the library maintainers might not want&#xB2;. The third solution means that the consumers of the library have to know about the problem and set it to a reasonable number to avoid LOH allocations. No, it seems like the fourth solution is the most likely to get accepted and has the least possibility of breaking existing functionality.</p><p>All we need is a number that is lower than 85,000, normally something like 84,000 would have been perfectly suitable. However, a few weeks prior to discovering this problem I was poking around <a href="https://referencesource.microsoft.com/">Reference Source</a> (investigating a different issue) when I stumbled across <a href="https://referencesource.microsoft.com/#mscorlib/system/io/stream.cs,53">this&#xA0;gem</a>:-</p><iframe src width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5bc56da17854a3be9aa0d5e37336a6cf/href">https://medium.com/media/5bc56da17854a3be9aa0d5e37336a6cf/href</a></iframe><p>Windows memory pages are <a href="https://blogs.msdn.microsoft.com/tims/2010/10/28/pdc10-mysteries-of-windows-memory-management-revealed-part-one/">4,096 bytes in size</a>, so picking a multiple of that which falls under the LOH threshold (85,000 bytes) makes complete sense. Time to fork, branch, <a href="https://github.com/aws/aws-sdk-net/issues/894">create an issue</a>, and make a <a href="https://github.com/aws/aws-sdk-net/pull/899">pull&#xA0;request</a>.</p><p>Luckily, we can make the change locally&#xB3; and see what the benefits are. Statistics for one hundred uploads of the same image via PutObject:-</p><iframe src width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/411e40a3134f9170f6a45a69e31296ac/href">https://medium.com/media/411e40a3134f9170f6a45a69e31296ac/href</a></iframe><h3>Idle hands</h3><p>Whilst waiting for my pull request to be reviewed I decided to poke around the AWS S3 documentation and I stumbled across the concept of <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html">pre-signed URLs</a>. That sounds interesting! Creating V2 of the uploader:-</p><iframe src width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9ab7508ba03a714cfe3e1150fef36913/href">https://medium.com/media/9ab7508ba03a714cfe3e1150fef36913/href</a></iframe><p>We see it has the following statistics when uploading the same file one hundred&#xA0;times:-</p><iframe src width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/98f289c65d76d26f1b56c0e45edd7f12/href">https://medium.com/media/98f289c65d76d26f1b56c0e45edd7f12/href</a></iframe><p>That is pretty awesome, and all we actually had to do to achieve that gain was read the documentation! Well, not true, you have the benefit of reading a summarised article with all the juicy bits. The work you see here took place over the course of a week, slotted in between client&#xA0;work.</p><p>There is a small downside to using GetPreSignedURL in that if the GetPreSignedUrlRequest is modified and the WebRequest is not modified accordingly then AWS will return <a href="https://en.wikipedia.org/wiki/HTTP_403">HTTP 403 Forbidden</a> (e.g. removing the XAmzAclHeader on the WebRequest). This is because the client-side hash and the server-side hashes no longer&#xA0;match.</p><h3>Just one more&#xA0;thing</h3><p>Thanks to my <a href="https://medium.com/@indy_singh/strings-are-evil-a803d05e5ce3">last article</a> I have learnt what <a href="https://xkcd.com/356/">nerd sniping</a> is&#x200A;&#x2014;&#x200A;something I do to myself quite a lot. At this stage I was feeling that giddiness about what else could be shaved off, I was wholly looking at the 0.4 MB remaining on the LOH. Again, dotTrace points us in the direction of code path causing that 0.4 MB allocation to the&#xA0;LOH:-</p><figure><img alt src="https://hackernoon.com/hn-images/0*C7evhJnmLT0KuYif.JPG"></figure><p>Yikes, <a href="https://referencesource.microsoft.com/#mscorlib/system/collections/hashtable.cs,735">that looks fairly serious</a>. Quietly backing away and trying a different tact; we know a pre-signed URL looks something like&#xA0;this:-</p><pre><code class="language-markup"><a href="https://##bucket_name##.s3.##region_name##.amazonaws.com/##path##/##file_name##?X-Amz-Expires=300&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=##access_key##/20180613/##region_name##/s3/aws4_request&amp;X-Amz-Date=20180613T233349Z&amp;X-Amz-SignedHeaders=host;x-amz-acl&amp;X-Amz-Signature=6bbcb0f802ad86022674e827d574b7a34a00ba76cd1411016c3581ba27fa5450">https://##bucket_name##.s3.##region_name##.amazonaws.com/##path##/##file_name##?X-Amz-Expires=300&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=##access_key##/20180613/##region_name##/s3/aws4_request&amp;X-Amz-Date=20180613T233349Z&amp;X-Amz-SignedHeaders=host;x-amz-acl&amp;X-Amz-Signature=6bbcb0f802ad86022674e827d574b7a34a00ba76cd1411016c3581ba27fa5450</a></code></pre><p>We should be able to generate that URL ourselves as AWS has very kindly published their <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html#signing-request-intro">signing process</a>. At this point I will admit that I was ready to accept defeat and just leave the 0.4 MB kicking about on the LOH. I really did not feel like the reams of code I was about to write to <em>possibly</em> eliminate that remaining 0.4 MB was going to be worth&#xA0;it.</p><p>That was until I spotted <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-examples-using-sdks.html#sig-v4-examples-using-sdk-dotnet">an example</a> of what I wanted. With considerably less effort required on my part; V3 was&#xA0;born:-</p><iframe src width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c4e3c000a9cd56ef26b0ac0739234bbf/href">https://medium.com/media/c4e3c000a9cd56ef26b0ac0739234bbf/href</a></iframe><p>V3 was just an experiment to see what was possible, given how small the gains and how much code there is to maintain it is not something we would actually use in production code. The discovery of pre-signed URLs is the main win&#xA0;here:-</p><iframe src width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/30bffa1808cbbb00853f1f420941a214/href">https://medium.com/media/30bffa1808cbbb00853f1f420941a214/href</a></iframe><p>Meanwhile my pull request had been <a href="https://github.com/aws/aws-sdk-net/pull/899#event-1548404894">merged</a> and released in version 3.3.21.19 of <a href="https://www.nuget.org/packages/AWSSDK.Core/">AWSSDK.Core</a>. Quick overview of the timeline:-</p><ol><li>2018&#x2013;03&#x2013;07&#x200A;&#x2014;&#x200A;Issue created on the aws-sdk-net repository</li><li>2018&#x2013;03&#x2013;13&#x200A;&#x2014;&#x200A;Pull request sent&#xA0;in</li><li>2018&#x2013;03&#x2013;29&#x200A;&#x2014;&#x200A;Pull request&#xA0;merged</li><li>2018&#x2013;03&#x2013;29&#x200A;&#x2014;&#x200A;New version of AWSSDK.Core released to&#xA0;NuGet</li></ol><p>I love open&#xA0;source.</p><h3>TLDR&#x200A;&#x2014;&#x200A;Give me the good&#xA0;stuff</h3><p>Versions of AWSSDK.Core below 3.3.21.19 caused a fixed cost of 0.3 MB per invocation of PutObject on the AWS S3&#xA0;.NET client. This was rectified in versions 3.3.21.19 and above. For particularly hot code paths, it is worth exploring the use of GetPreSignedURL on the AWS S3&#xA0;.NET client as that dropped LOH allocations by 98% in our context and use&#xA0;case.</p><p>Find me <a href="https://twitter.com/indy_singh_uk">Twitter</a>, <a href="https://uk.linkedin.com/in/indy-singh-uk">LinkedIn</a>, or&#xA0;<a href="https://github.com/indy-singh">GitHub</a>.</p><h3>Footnotes</h3><p>&#xB9; Another reason <em>may</em> be that WinDbg still scares&#xA0;me.</p><p>&#xB2; That being said a <a href="https://github.com/aws/aws-sdk-net/issues/971">recent conversation has been started</a> to take advantage of&#xA0;.NET Core&#xA0;goodness</p><p>&#xB3; Make sure to build in release unlike a <em>certain</em> someone&#x200A;&#x2014;&#x200A;okay it was&#xA0;me</p><p><em>Originally published at&#xA0;</em><a href="https://dev.to/indy_singh_uk/its-not-your-code-vol-i-17n0"><em>dev.to</em></a><em>.</em></p><figure><a href="https://goo.gl/w4Pbea"><img alt src="https://hackernoon.com/hn-images/1*PZjwR1Nbluff5IMI6Y1T6g@2x.png"></a></figure>                <div class="archive-tags">                                        <a class="tag" href="https://hackernoon.com/tagged/software-development">Software Development</a>                                        <a class="tag" href="https://hackernoon.com/tagged/programming">Programming</a>                                        <a class="tag" href="https://hackernoon.com/tagged/coding">Coding</a>                                        <a class="tag" href="https://hackernoon.com/tagged/csharp">Csharp</a>                                        <a class="tag" href="https://hackernoon.com/tagged/aws">Aws</a>                  </div>                <div class="twitter-discussion">          <a target="_blank" href="https://twitter.com/search?q=https%3A%2F%2Fhackernoon.com%2Fits-not-your-code-vol-i-c06fac8784df">Continue the discussion <i class="fab fa-twitter"></i></a>        </div>